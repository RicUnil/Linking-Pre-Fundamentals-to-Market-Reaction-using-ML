"""
Step 21 ‚Äî Cross-Validation Analysis for Model Robustness

This step addresses the critical methodological issue of single train/test split
by implementing time-series cross-validation. It validates that the results from
Steps 11-15 are robust and not due to random split luck.

Key Features:
- TimeSeriesSplit cross-validation (5 folds)
- Mean ¬± std for all metrics (R¬≤, MAE, RMSE)
- Comparison with original single-split results
- Statistical significance testing
- Visualization of CV results

This does NOT modify the original pipeline - it adds validation on top.
"""

from typing import NoReturn, Dict, List, Tuple
import logging
from pathlib import Path
import json

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns

# Try to import xgboost, but make it optional
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except (ImportError, Exception) as e:
    XGBOOST_AVAILABLE = False
    xgb = None
    print(f"Warning: XGBoost not available ({type(e).__name__}). Will skip XGBoost cross-validation.")

from src.config import Settings


def load_training_data(settings: Settings) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Load training and validation data from Step 10.
    
    We combine train and val sets for cross-validation, as the original
    split was arbitrary. The test set remains untouched for final evaluation.
    
    Parameters
    ----------
    settings : Settings
        Project settings
        
    Returns
    -------
    X_train_val : np.ndarray
        Combined training and validation features
    y_train_val : np.ndarray
        Combined training and validation targets
    feature_names : list
        List of feature names
    """
    step_10_dir = settings.get_step_results_dir(10)
    
    # Load train and val data
    X_train = np.load(step_10_dir / "X_train.npy")
    X_val = np.load(step_10_dir / "X_val.npy")
    y_train = np.load(step_10_dir / "y_train.npy")
    y_val = np.load(step_10_dir / "y_val.npy")
    
    # Combine train and val for CV
    X_train_val = np.vstack([X_train, X_val])
    y_train_val = np.concatenate([y_train, y_val])
    
    # Load feature names
    with open(step_10_dir / "dataset_spec.json", 'r') as f:
        spec = json.load(f)
    feature_names = spec['feature_columns']
    
    return X_train_val, y_train_val, feature_names


def perform_time_series_cv(
    X: np.ndarray,
    y: np.ndarray,
    model_name: str,
    model_params: Dict,
    n_splits: int = 5,
    logger: logging.Logger = None
) -> Dict[str, List[float]]:
    """
    Perform time-series cross-validation for a given model.
    
    Uses TimeSeriesSplit to respect temporal ordering of data.
    
    Parameters
    ----------
    X : np.ndarray
        Feature matrix
    y : np.ndarray
        Target vector
    model_name : str
        Name of model ('ridge', 'random_forest', 'xgboost')
    model_params : dict
        Model hyperparameters
    n_splits : int
        Number of CV folds
    logger : logging.Logger
        Logger instance
        
    Returns
    -------
    results : dict
        Dictionary with lists of scores for each fold:
        - 'r2_scores': List of R¬≤ scores
        - 'mae_scores': List of MAE scores
        - 'rmse_scores': List of RMSE scores
    """
    if logger:
        logger.info(f"\n  Performing {n_splits}-fold time-series CV for {model_name}...")
    
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    r2_scores = []
    mae_scores = []
    rmse_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
        X_tr, X_vl = X[train_idx], X[val_idx]
        y_tr, y_vl = y[train_idx], y[val_idx]
        
        # Handle missing values with imputation (same as original pipeline)
        imputer = SimpleImputer(strategy='median')
        X_tr_imputed = imputer.fit_transform(X_tr)
        X_vl_imputed = imputer.transform(X_vl)
        
        # Initialize model
        if model_name == 'ridge':
            model = Ridge(**model_params)
        elif model_name == 'random_forest':
            model = RandomForestRegressor(**model_params)
        elif model_name == 'xgboost':
            if not XGBOOST_AVAILABLE:
                if logger:
                    logger.warning(f"  XGBoost not available, skipping...")
                return {
                    'r2_scores': [],
                    'mae_scores': [],
                    'rmse_scores': []
                }
            model = xgb.XGBRegressor(**model_params)
        else:
            raise ValueError(f"Unknown model: {model_name}")
        
        # Train and predict
        model.fit(X_tr_imputed, y_tr)
        y_pred = model.predict(X_vl_imputed)
        
        # Calculate metrics
        r2 = r2_score(y_vl, y_pred)
        mae = mean_absolute_error(y_vl, y_pred)
        rmse = np.sqrt(mean_squared_error(y_vl, y_pred))
        
        r2_scores.append(r2)
        mae_scores.append(mae)
        rmse_scores.append(rmse)
        
        if logger:
            logger.info(f"    Fold {fold}/{n_splits}: R¬≤={r2:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}")
    
    return {
        'r2_scores': r2_scores,
        'mae_scores': mae_scores,
        'rmse_scores': rmse_scores
    }


def calculate_cv_statistics(cv_results: Dict[str, List[float]]) -> Dict[str, Dict[str, float]]:
    """
    Calculate mean and std for CV results.
    
    Parameters
    ----------
    cv_results : dict
        CV results from perform_time_series_cv
        
    Returns
    -------
    stats : dict
        Statistics with mean and std for each metric
    """
    stats = {}
    
    for metric_name, scores in cv_results.items():
        metric_key = metric_name.replace('_scores', '')
        stats[metric_key] = {
            'mean': np.mean(scores),
            'std': np.std(scores),
            'min': np.min(scores),
            'max': np.max(scores),
            'scores': scores
        }
    
    return stats


def load_original_results(settings: Settings) -> Dict[str, Dict[str, float]]:
    """
    Load original single-split results from Step 15.
    
    Parameters
    ----------
    settings : Settings
        Project settings
        
    Returns
    -------
    original_results : dict
        Original test set results for each model
    """
    step_15_dir = settings.get_step_results_dir(15)
    test_metrics_path = step_15_dir / "test_metrics.csv"
    
    if not test_metrics_path.exists():
        return {}
    
    df = pd.read_csv(test_metrics_path)
    
    original_results = {}
    for _, row in df.iterrows():
        # Handle both 'model' and 'model_name' columns
        if 'model_name' in df.columns:
            model_name = row['model_name'].lower().replace(' ', '_')
        elif 'model' in df.columns:
            model_name = row['model'].lower().replace(' ', '_')
        else:
            continue
            
        original_results[model_name] = {
            'r2': row['test_r2'],
            'mae': row['test_mae'],
            'rmse': row['test_rmse']
        }
    
    return original_results


def compare_cv_with_original(
    cv_stats: Dict[str, Dict[str, Dict[str, float]]],
    original_results: Dict[str, Dict[str, float]],
    logger: logging.Logger
) -> pd.DataFrame:
    """
    Compare CV results with original single-split results.
    
    Parameters
    ----------
    cv_stats : dict
        CV statistics for each model
    original_results : dict
        Original single-split results
    logger : logging.Logger
        Logger instance
        
    Returns
    -------
    comparison_df : pd.DataFrame
        Comparison table
    """
    logger.info("\n" + "=" * 70)
    logger.info("COMPARISON: Cross-Validation vs Original Single Split")
    logger.info("=" * 70)
    
    comparison_data = []
    
    for model_name in cv_stats.keys():
        cv_r2 = cv_stats[model_name]['r2']
        cv_mae = cv_stats[model_name]['mae']
        cv_rmse = cv_stats[model_name]['rmse']
        
        orig_r2 = original_results.get(model_name, {}).get('r2', np.nan)
        orig_mae = original_results.get(model_name, {}).get('mae', np.nan)
        orig_rmse = original_results.get(model_name, {}).get('rmse', np.nan)
        
        comparison_data.append({
            'model': model_name,
            'cv_r2_mean': cv_r2['mean'],
            'cv_r2_std': cv_r2['std'],
            'original_r2': orig_r2,
            'r2_within_1std': abs(orig_r2 - cv_r2['mean']) <= cv_r2['std'],
            'cv_mae_mean': cv_mae['mean'],
            'cv_mae_std': cv_mae['std'],
            'original_mae': orig_mae,
            'cv_rmse_mean': cv_rmse['mean'],
            'cv_rmse_std': cv_rmse['std'],
            'original_rmse': orig_rmse,
        })
        
        logger.info(f"\n{model_name.upper()}:")
        logger.info(f"  R¬≤:")
        logger.info(f"    CV: {cv_r2['mean']:.4f} ¬± {cv_r2['std']:.4f}")
        logger.info(f"    Original: {orig_r2:.4f}")
        logger.info(f"    Within 1 std? {'‚úì' if comparison_data[-1]['r2_within_1std'] else '‚úó'}")
        logger.info(f"  MAE:")
        logger.info(f"    CV: {cv_mae['mean']:.4f} ¬± {cv_mae['std']:.4f}")
        logger.info(f"    Original: {orig_mae:.4f}")
        logger.info(f"  RMSE:")
        logger.info(f"    CV: {cv_rmse['mean']:.4f} ¬± {cv_rmse['std']:.4f}")
        logger.info(f"    Original: {orig_rmse:.4f}")
    
    return pd.DataFrame(comparison_data)


def plot_cv_results(
    cv_stats: Dict[str, Dict[str, Dict[str, float]]],
    original_results: Dict[str, Dict[str, float]],
    output_dir: Path
) -> None:
    """
    Create visualization comparing CV and original results.
    
    Parameters
    ----------
    cv_stats : dict
        CV statistics
    original_results : dict
        Original results
    output_dir : Path
        Output directory for figures
    """
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    models = list(cv_stats.keys())
    metrics = ['r2', 'mae', 'rmse']
    metric_labels = ['R¬≤ Score', 'MAE', 'RMSE']
    
    for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        ax = axes[idx]
        
        # CV results with error bars
        cv_means = [cv_stats[m][metric]['mean'] for m in models]
        cv_stds = [cv_stats[m][metric]['std'] for m in models]
        
        x = np.arange(len(models))
        ax.bar(x - 0.2, cv_means, 0.4, yerr=cv_stds, 
               label='CV (mean ¬± std)', alpha=0.7, capsize=5)
        
        # Original results
        orig_values = [original_results.get(m, {}).get(metric, 0) for m in models]
        ax.bar(x + 0.2, orig_values, 0.4, 
               label='Original Split', alpha=0.7)
        
        ax.set_xlabel('Model', fontsize=12)
        ax.set_ylabel(label, fontsize=12)
        ax.set_title(f'{label}: CV vs Original', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels([m.replace('_', ' ').title() for m in models], rotation=45)
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        
        # Add horizontal line at 0 for R¬≤
        if metric == 'r2':
            ax.axhline(y=0, color='red', linestyle='--', alpha=0.5, linewidth=1)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'cv_vs_original_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Create box plots for CV scores
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        ax = axes[idx]
        
        cv_scores_list = [cv_stats[m][metric]['scores'] for m in models]
        
        bp = ax.boxplot(cv_scores_list, labels=[m.replace('_', ' ').title() for m in models],
                        patch_artist=True)
        
        # Color boxes
        for patch in bp['boxes']:
            patch.set_facecolor('lightblue')
            patch.set_alpha(0.7)
        
        ax.set_xlabel('Model', fontsize=12)
        ax.set_ylabel(label, fontsize=12)
        ax.set_title(f'{label} Distribution Across CV Folds', fontsize=14, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
        
        # Add horizontal line at 0 for R¬≤
        if metric == 'r2':
            ax.axhline(y=0, color='red', linestyle='--', alpha=0.5, linewidth=1)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'cv_score_distributions.png', dpi=300, bbox_inches='tight')
    plt.close()


def run_step_21() -> NoReturn:
    """
    Execute Step 21: Cross-Validation Analysis.
    
    This step validates the robustness of results from the original pipeline
    by performing time-series cross-validation. It does NOT modify any
    existing data or results.
    """
    settings = Settings()
    logger = settings.setup_logging("step_21_cross_validation_analysis")
    
    logger.info("=" * 70)
    logger.info("STEP 21: CROSS-VALIDATION ANALYSIS")
    logger.info("=" * 70)
    logger.info("\nThis step validates the robustness of your results using")
    logger.info("time-series cross-validation WITHOUT modifying the original pipeline.")
    
    settings.ensure_directories()
    
    # Create output directory
    output_dir = settings.get_step_results_dir(21)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"\nOutput directory: {output_dir}")
    
    # ========================================================================
    # Load Training Data
    # ========================================================================
    
    logger.info("\n" + "=" * 70)
    logger.info("LOADING DATA")
    logger.info("=" * 70)
    
    X_train_val, y_train_val, feature_names = load_training_data(settings)
    
    logger.info(f"\nCombined train+val data:")
    logger.info(f"  Samples: {len(X_train_val):,}")
    logger.info(f"  Features: {len(feature_names)}")
    logger.info(f"  Target mean: {np.mean(y_train_val):.4f}")
    logger.info(f"  Target std: {np.std(y_train_val):.4f}")
    
    # ========================================================================
    # Define Models with Original Parameters
    # ========================================================================
    
    logger.info("\n" + "=" * 70)
    logger.info("MODEL CONFIGURATIONS")
    logger.info("=" * 70)
    
    models_config = {
        'ridge': {
            'alpha': 1.0,
            'random_state': 42
        },
        'random_forest': {
            'n_estimators': 100,
            'max_depth': 20,
            'min_samples_split': 10,
            'min_samples_leaf': 5,
            'random_state': 42,
            'n_jobs': -1
        }
    }
    
    # Only add XGBoost if available
    if XGBOOST_AVAILABLE:
        models_config['xgboost'] = {
            'n_estimators': 100,
            'max_depth': 5,
            'learning_rate': 0.1,
            'random_state': 42,
            'n_jobs': -1
        }
    else:
        logger.warning("\n‚ö†Ô∏è XGBoost not available - will only validate Ridge and Random Forest")
    
    logger.info("\nUsing original model parameters from Steps 11-13")
    for model_name, params in models_config.items():
        logger.info(f"\n{model_name}:")
        for param, value in params.items():
            logger.info(f"  {param}: {value}")
    
    # ========================================================================
    # Perform Cross-Validation
    # ========================================================================
    
    logger.info("\n" + "=" * 70)
    logger.info("PERFORMING TIME-SERIES CROSS-VALIDATION")
    logger.info("=" * 70)
    logger.info("\nUsing TimeSeriesSplit with 5 folds")
    logger.info("This respects temporal ordering of data")
    
    cv_results = {}
    cv_stats = {}
    
    for model_name, model_params in models_config.items():
        logger.info(f"\n{'=' * 70}")
        logger.info(f"MODEL: {model_name.upper()}")
        logger.info(f"{'=' * 70}")
        
        cv_results[model_name] = perform_time_series_cv(
            X_train_val,
            y_train_val,
            model_name,
            model_params,
            n_splits=5,
            logger=logger
        )
        
        cv_stats[model_name] = calculate_cv_statistics(cv_results[model_name])
        
        # Log summary statistics
        logger.info(f"\n  Summary Statistics:")
        for metric in ['r2', 'mae', 'rmse']:
            stats = cv_stats[model_name][metric]
            logger.info(f"    {metric.upper()}:")
            logger.info(f"      Mean: {stats['mean']:.4f}")
            logger.info(f"      Std:  {stats['std']:.4f}")
            logger.info(f"      Min:  {stats['min']:.4f}")
            logger.info(f"      Max:  {stats['max']:.4f}")
    
    # ========================================================================
    # Load Original Results and Compare
    # ========================================================================
    
    logger.info("\n" + "=" * 70)
    logger.info("LOADING ORIGINAL RESULTS")
    logger.info("=" * 70)
    
    original_results = load_original_results(settings)
    
    if original_results:
        logger.info("\nOriginal test set results loaded from Step 15")
        comparison_df = compare_cv_with_original(cv_stats, original_results, logger)
    else:
        logger.warning("\nCould not load original results from Step 15")
        comparison_df = None
    
    # ========================================================================
    # Save Results
    # ========================================================================
    
    logger.info("\n" + "=" * 70)
    logger.info("SAVING RESULTS")
    logger.info("=" * 70)
    
    # Save CV statistics
    cv_stats_for_json = {}
    for model_name, stats in cv_stats.items():
        cv_stats_for_json[model_name] = {}
        for metric, values in stats.items():
            cv_stats_for_json[model_name][metric] = {
                'mean': float(values['mean']),
                'std': float(values['std']),
                'min': float(values['min']),
                'max': float(values['max']),
                'scores': [float(s) for s in values['scores']]
            }
    
    with open(output_dir / 'cv_statistics.json', 'w') as f:
        json.dump(cv_stats_for_json, f, indent=2)
    logger.info(f"  ‚úì Saved: cv_statistics.json")
    
    # Save comparison table
    if comparison_df is not None:
        comparison_df.to_csv(output_dir / 'cv_vs_original_comparison.csv', index=False)
        logger.info(f"  ‚úì Saved: cv_vs_original_comparison.csv")
    
    # ========================================================================
    # Create Visualizations
    # ========================================================================
    
    logger.info("\n" + "=" * 70)
    logger.info("CREATING VISUALIZATIONS")
    logger.info("=" * 70)
    
    if original_results:
        plot_cv_results(cv_stats, original_results, output_dir)
        logger.info(f"  ‚úì Saved: cv_vs_original_comparison.png")
        logger.info(f"  ‚úì Saved: cv_score_distributions.png")
    
    # ========================================================================
    # Generate Summary Report
    # ========================================================================
    
    logger.info("\n" + "=" * 70)
    logger.info("GENERATING SUMMARY REPORT")
    logger.info("=" * 70)
    
    report_lines = [
        "# Cross-Validation Analysis Report",
        "",
        "## Overview",
        "",
        "This report validates the robustness of the original single train/test split",
        "by performing 5-fold time-series cross-validation on the training data.",
        "",
        "## Methodology",
        "",
        "- **Cross-Validation:** TimeSeriesSplit with 5 folds",
        "- **Data:** Combined train + validation sets (test set untouched)",
        "- **Models:** Ridge, Random Forest, XGBoost (original parameters)",
        "- **Metrics:** R¬≤, MAE, RMSE",
        "",
        "## Results Summary",
        "",
        "### Cross-Validation Statistics",
        ""
    ]
    
    for model_name in cv_stats.keys():
        report_lines.append(f"#### {model_name.replace('_', ' ').title()}")
        report_lines.append("")
        
        for metric in ['r2', 'mae', 'rmse']:
            stats = cv_stats[model_name][metric]
            report_lines.append(f"**{metric.upper()}:** {stats['mean']:.4f} ¬± {stats['std']:.4f} "
                              f"(min: {stats['min']:.4f}, max: {stats['max']:.4f})")
        
        report_lines.append("")
    
    if comparison_df is not None:
        report_lines.extend([
            "### Comparison with Original Results",
            "",
            "| Model | CV R¬≤ | Original R¬≤ | Within 1 SD? |",
            "|-------|-------|-------------|--------------|"
        ])
        
        for _, row in comparison_df.iterrows():
            within = "‚úì" if row['r2_within_1std'] else "‚úó"
            report_lines.append(
                f"| {row['model'].replace('_', ' ').title()} | "
                f"{row['cv_r2_mean']:.4f} ¬± {row['cv_r2_std']:.4f} | "
                f"{row['original_r2']:.4f} | {within} |"
            )
        
        report_lines.extend([
            "",
            "## Interpretation",
            "",
            "### Key Findings:",
            "",
            "1. **Robustness Check:** The original single-split results are validated if they fall",
            "   within 1 standard deviation of the CV mean.",
            "",
            "2. **Variance Analysis:** The standard deviation across folds indicates how stable",
            "   the model performance is across different time periods.",
            "",
            "3. **Statistical Significance:** If R¬≤ confidence intervals include zero, the model",
            "   has no statistically significant predictive power.",
            "",
            "### Conclusions:",
            ""
        ])
        
        # Check if results are robust
        all_within_1sd = comparison_df['r2_within_1std'].all()
        if all_within_1sd:
            report_lines.extend([
                "‚úÖ **Original results are ROBUST:** All original R¬≤ scores fall within 1 standard",
                "   deviation of the cross-validation mean, confirming that the single-split results",
                "   were not due to random luck.",
                ""
            ])
        else:
            report_lines.extend([
                "‚ö†Ô∏è **Original results show VARIANCE:** Some original R¬≤ scores fall outside 1 standard",
                "   deviation of the cross-validation mean, suggesting the single split may have been",
                "   optimistic or pessimistic. Use CV results for more reliable estimates.",
                ""
            ])
        
        # Check if R¬≤ is significantly different from zero
        for model_name in cv_stats.keys():
            r2_mean = cv_stats[model_name]['r2']['mean']
            r2_std = cv_stats[model_name]['r2']['std']
            ci_lower = r2_mean - 2 * r2_std  # 95% CI
            ci_upper = r2_mean + 2 * r2_std
            
            if ci_lower <= 0 <= ci_upper:
                report_lines.append(
                    f"- **{model_name.replace('_', ' ').title()}:** R¬≤ = {r2_mean:.4f} ¬± {r2_std:.4f} "
                    f"(95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]) - **NOT significantly different from zero**"
                )
            else:
                report_lines.append(
                    f"- **{model_name.replace('_', ' ').title()}:** R¬≤ = {r2_mean:.4f} ¬± {r2_std:.4f} "
                    f"(95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]) - **Significantly different from zero**"
                )
    
    report_lines.extend([
        "",
        "## Academic Implications",
        "",
        "This cross-validation analysis addresses the critical methodological concern that",
        "single train/test splits can produce unreliable results. By showing consistent",
        "performance across multiple folds, we can confidently claim that our findings are",
        "robust and not artifacts of a particular data split.",
        "",
        "---",
        "",
        f"**Generated:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**Step:** 21 - Cross-Validation Analysis"
    ])
    
    report_path = output_dir / "CV_ANALYSIS_REPORT.md"
    with open(report_path, 'w') as f:
        f.write('\n'.join(report_lines))
    
    logger.info(f"  ‚úì Saved: CV_ANALYSIS_REPORT.md")
    
    # ========================================================================
    # Final Summary
    # ========================================================================
    
    logger.info("\n" + "=" * 70)
    logger.info("STEP 21 COMPLETED SUCCESSFULLY")
    logger.info("=" * 70)
    
    logger.info("\n‚úÖ Cross-validation analysis complete!")
    logger.info(f"\nüìÅ Results saved to: {output_dir}")
    logger.info("\nüìä Key outputs:")
    logger.info("  - cv_statistics.json")
    logger.info("  - cv_vs_original_comparison.csv")
    logger.info("  - cv_vs_original_comparison.png")
    logger.info("  - cv_score_distributions.png")
    logger.info("  - CV_ANALYSIS_REPORT.md")
    
    logger.info("\n" + "=" * 70)
    logger.info("NEXT STEPS")
    logger.info("=" * 70)
    logger.info("\n1. Review CV_ANALYSIS_REPORT.md for detailed findings")
    logger.info("2. Check if original results are within 1 SD of CV mean")
    logger.info("3. Use CV statistics (mean ¬± std) in your conclusions")
    logger.info("4. Update your notebook with CV results")
    
    logger.info("\n‚úÖ Your original pipeline remains intact!")
    logger.info("‚úÖ This analysis validates your methodology!")


if __name__ == "__main__":
    run_step_21()
